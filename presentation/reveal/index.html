<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
      Are Frontend Developers at Risk? AI Is Already Writing HTML for Us
    </title>
    <meta
      name="description"
      content="A live case study on building a production-grade digital business card platform using AI-assisted development workflow"
    />
    <meta name="author" content="Łukasz Piotr Łuczak" />

    <link rel="stylesheet" href="./reset.css" />
    <link rel="stylesheet" href="./reveal.css" />
    <link rel="stylesheet" href="./theme.css" />
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <!-- Title Slide -->
        <section
          data-markdown
          data-separator="^\n---\n$"
          data-separator-vertical="^\n--\n$"
        >
          <textarea data-template>
            # Are Frontend Developers at Risk?

            ## AI Is Already Writing HTML for Us

            **Łukasz Piotr Łuczak**
            Software Architect • Consultant • Researcher

            DevFest Łódź 2025

            <aside class="notes">
              Hi, I'm Łukasz Piotr Łuczak. Three Ł's. It's easier to show you my online business card than to spell my name.

              Except... I don't have one. I never had time to build it.

              Meanwhile, we keep hearing "AI will replace us." So I thought: let's test that claim. Let's see if AI can build my business card from scratch — a real, production-grade platform.

              Oh, and one more thing: these speaker notes you're NOT seeing? Also generated by AI. I'm literally using AI to explain how I used AI. Meta, right?
            </aside>

            ---

            # The Problem

            - No online business card
            - Manual networking is inefficient
            - Scattered social profiles

            <aside class="notes">
              As a consultant and speaker, I need to network efficiently. Give people one link, they get everything: bio, contact, talks, publications.

              But like many developers, I kept postponing this "simple" project. Too busy. Too many other priorities.

              Sound familiar?
            </aside>

            ---

            # The Premise

            **"AI will replace us"**

            Let's test this claim:
            - Can AI build a production-ready platform?
            - From scratch to deployment?
            - No manual coding?

            <aside class="notes">
              We hear this constantly: "AI will replace developers." Twitter debates, conference talks, existential dread.

              So instead of arguing, I decided to run an experiment. Can AI actually build a production-ready platform? From zero to deployment? No manual coding?

              Let's find out.
            </aside>

            ---

            # The Project

            **Digital Business Card Platform**

            - Single-link contact hub
            - Bilingual (EN/PL)
            - Mobile-first, SEO-optimized
            - Tabs: Me, Consulting, Contact, Events, Publications
            - Production-grade standards

            <aside class="notes">
              Not a toy project. Not a landing page. A real platform with:
              - Bilingual support (English and Polish)
              - SEO and structured data
              - Accessibility standards
              - Security headers
              - CI/CD pipeline
              - Production deployment

              This is what production means.
            </aside>

            ---

            # The AI-Assisted Process

            **Artifacts Pipeline:**

            1. Information Architecture
            2. Brand & Visual Identity
            3. Design Profile (design-profile.json)
            4. Tailwind Theme CSS
            5. HTML Prototypes
            6. Repository DX Setup
            7. Architecture (NestJS + Astro)
            8. Implementation

            <aside class="notes">
              This wasn't "ChatGPT, write me a website."

              It was a structured, phased workflow — the same process I'd use with a human team:
              Discovery → Brand identity → Design system → Prototypes → Architecture → Implementation

              Each phase produced artifacts that fed into the next. Information architecture document. Design profile JSON. Tailwind theme. HTML prototypes.

              This is how you build real software. With or without AI.
            </aside>

            ---

            # Key Constraint

            **No manual fixes — only prompts**

            - Every change via AI prompt
            - Follow-up prompts for refinement
            - Execution logs replaced chat logs
            - Quality gates made issues visible

            <aside class="notes">
              Here's the rule I set for myself: I intentionally did not manually fix the code. Only prompts and follow-up prompts.

              If CI failed, I couldn't just edit the file. I had to describe the problem to AI and let it fix it.

              Once AI had write access to the repository, execution logs replaced chat logs. No more copy-paste. Just prompts and git commits.

              This constraint forced me to test AI's real-world capability, not its ability to generate code snippets.
            </aside>

            ---

            # Friction Point 1

            **pnpm Version Conflict**

            - CI specified `version: 9`
            - package.json specified `pnpm@9.15.0`
            - pnpm/action-setup@v4 detected conflict

            **Solution:**
            Remove CI version, use package.json as single source of truth

            <aside class="notes">
              First failure. CI blew up: "ERR_PNPM_BAD_PM_VERSION."

              Why? The workflow specified pnpm version 9. The package.json specified pnpm 9.15.0. Both are correct, but they conflict.

              AI debugged it: remove the redundant version from CI, use package.json as single source of truth.

              Lesson: CI didn't break the build — it revealed a missing assumption about how pnpm/action-setup works.
            </aside>

            ---

            # Friction Point 2

            **Missing pnpm Lockfile**

            - CI ran pnpm install
            - No pnpm-lock.yaml committed
            - Non-deterministic builds

            **Solution:**
            Generate and commit pnpm-lock.yaml

            <aside class="notes">
              Second failure. "pnpm install" ran, but builds weren't deterministic.

              Why? No lockfile. Dependencies floating. Classic mistake.

              AI generated and committed pnpm-lock.yaml. Fixed.

              Lesson: Reproducibility requires explicit state. AI knows this. Do you always remember?
            </aside>

            ---

            # Friction Point 3

            **Prettier Format Check Failures**

            - AI-generated code wasn't formatted
            - CI format check failed

            **Solution:**
            Run pnpm run format:fix before commit

            <aside class="notes">
              Third failure. Prettier check failed.

              AI-generated code was syntactically correct. But it didn't match the project's formatting rules.

              Solution: run "pnpm run format:fix" before commit. Done.

              Lesson: Quality gates caught this immediately. Without them, the codebase would drift into chaos.
            </aside>

            ---

            # Friction Point 4

            **ESM/CommonJS Module Mismatch**

            - NestJS built as ESM
            - Astro SSR adapter expected CommonJS
            - Runtime crash: "Cannot use import outside module"

            **Solution:**
            Add build:postprocess to write package.json with type: commonjs

            <aside class="notes">
              Fourth failure. Runtime crash: "Cannot use import statement outside a module."

              Why? NestJS built as ESM. Astro SSR adapter expected CommonJS. Module system mismatch.

              This one required multiple prompts. AI debugged it, but it wasn't obvious. Complex runtime issues are still hard for AI.

              Solution: add a postprocess step to inject `{type: "commonjs"}` into the build output.
            </aside>

            ---

            # What Works Today

            - ✅ Bilingual SPA (EN/PL)
            - ✅ Tabs with icon + label
            - ✅ Light/Dark mode
            - ✅ SEO metadata + JSON-LD
            - ✅ Security headers (CSP, HSTS)
            - ✅ Contact form API (stub)
            - ✅ CI/CD quality gates
            - ✅ Coolify-ready deployment

            <aside class="notes">
              So what actually works?

              All of this. Bilingual SPA. SEO metadata. Security headers. Contact form API (stubbed, but structured). CI/CD quality gates. Deployment-ready.

              It's not pretty yet. The design needs polish. But it's functional, secure, and production-grade.

              And that's exactly what I asked for.
            </aside>

            ---

            # What's Intentionally Unfinished

            - ⏳ Contact form email integration (stub only)
            - ⏳ CAPTCHA (planned, GDPR-friendly)
            - ⏳ CSP hardening (remove unsafe-inline)
            - ⏳ Analytics (privacy-first, post-MVP)

            <aside class="notes">
              What's intentionally unfinished?

              Contact form email integration — it logs to console instead of sending emails. CAPTCHA — not implemented yet. CSP hardening — still allows unsafe-inline for now.

              The app works. It doesn't look good yet. And that's fine for this demo.

              The goal was to test AI's ability to scaffold production infrastructure, not to over-engineer every detail.
            </aside>

            ---

            # The Repository as the Product

            **Everything is public:**

            - All prompts (prompts/)
            - All conversation logs (conversations/)
            - All design artifacts (docs/)
            - All code (app/)
            - All CI failures and fixes

            **github.com/lukaszpiotrluczak/devfest-lodz-2015**

            <aside class="notes">
              The repository IS the product of this talk.

              Everything is public: all prompts, all conversation logs, all design artifacts, all code, all CI failures and fixes.

              You can trace the entire journey. See where AI succeeded. See where it struggled. See how I guided it.

              Full transparency. No cherry-picking. This is the real story.
            </aside>

            ---

            # Key Lessons

            **AI accelerates, but needs guidance**

            - AI is a powerful tool, not a replacement
            - Quality gates are essential
            - Humans steer strategy and architecture
            - Works best for structured, well-scoped tasks
            - Struggles with runtime debugging and edge cases

            <aside class="notes">
              So what did I learn?

              AI didn't replace me. It accelerated me. I still made every architectural decision — NestJS vs Express, Astro vs Next, build strategy, security headers.

              AI executed. I directed. Quality gates kept us honest.

              Works best for structured, well-scoped tasks. Struggles with runtime debugging and architectural edge cases.

              It's a powerful tool. Not a replacement.
            </aside>

            ---

            # The Honest Takeaway

            **Can AI replace frontend developers?**

            - ❌ Not for complex, production systems
            - ✅ For simple, well-defined tasks
            - ✅ As an accelerator with human oversight
            - ✅ When paired with quality gates

            **Developers who use AI > Developers who don't**

            <aside class="notes">
              Can AI replace frontend developers?

              No. Not for complex, production systems. Not yet. Maybe never.

              But can it make you faster? Absolutely. If you know how to guide it. If you have quality gates. If you understand what you're building.

              The real question isn't "Will AI replace us?" It's "Will developers who use AI effectively replace those who don't?"

              I think we know the answer.
            </aside>

            ---

            # Thank You

            **Explore the full repo:**

            github.com/lukaszpiotrluczak/devfest-lodz-2015

            **Connect:**

            - lukaszpiotrluczak.me
            - linkedin.com/in/lukaszpiotrluczak
            - github.com/lukaszpiotrluczak

            <aside class="notes">
              That's it. Thanks for listening.

              Explore the repo — everything is there. Prompts, logs, code, mistakes, fixes. The full story.

              Questions?
            </aside>
          </textarea>
        </section>
      </div>
    </div>

    <footer class="presentation-footer">
      <div class="footer-content">
        <span class="footer-title">AI-Assisted Development: A Case Study</span>
        <span class="footer-divider">•</span>
        <span class="footer-author">Łukasz Piotr Łuczak</span>
        <span class="footer-divider">•</span>
        <span class="footer-repo"
          >github.com/lukaszpiotrluczak/devfest-lodz-2015</span
        >
      </div>
    </footer>

    <script type="module">
      import Reveal from './reveal.esm.js';
      import Markdown from './markdown.esm.js';
      import Notes from './notes.esm.js';
      import Highlight from './highlight.esm.js';

      const deck = new Reveal({
        plugins: [Markdown, Notes, Highlight],
        hash: true,
        slideNumber: 'c/t',
        showSlideNumber: 'speaker',
        transition: 'slide',
        backgroundTransition: 'fade',
        center: true,
        width: 1280,
        height: 720,
        margin: 0.04,
        minScale: 0.2,
        maxScale: 2.0,
      });

      deck.initialize();
    </script>
  </body>
</html>
